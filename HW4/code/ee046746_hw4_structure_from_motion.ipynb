{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "\n",
    "\n",
    "## Homework 4 - Structure From Motion\n",
    "---\n",
    "\n",
    "### <a style='color:red'> Due Date: 25.8.2024 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "#### READ THIS CAREFULLY\n",
    "* Submission only in **pairs**.\n",
    "* **No handwritten submissions**.\n",
    "* You can choose your working environment:\n",
    "    * You can work in a `Jupyter Notebook`, locally with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or online on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "  * **Important**: Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime` $\\rightarrow$  `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    * You can work in a Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both also allow opening/editing Jupyter Notebooks.\n",
    "\n",
    "* Make sure you submit your exercise according to the requirements in the <a href=\"https://moodle.technion.ac.il/pluginfile.php/1823033/mod_resource/content/2/Homework%20guidelines.pdf\">\"Homework submission guidelines\"</a> file that appears in the course website (Moodle). \n",
    "            * **The code should run both on CPU and GPU without manual modifications**, require no special preparation and run on every computer.\n",
    "* Be precise, we expect on point answers.\n",
    "* Submission on the course website (Moodle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/python.png\" style=\"height:50px;display:inline\"> Python Libraries\n",
    "---\n",
    "\n",
    "* `numpy`\n",
    "* `matplotlib`\n",
    "* `opencv` (or `scikit-image`)\n",
    "* `scikit-learn`\n",
    "* `scipy`\n",
    "* Anything else you need (`os`, `pandas`, `csv`, `json`,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tk in c:\\users\\rom\\anaconda3\\envs\\ml2\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: PyQt5 in c:\\users\\rom\\anaconda3\\envs\\ml2\\lib\\site-packages (5.15.10)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.13 in c:\\users\\rom\\anaconda3\\envs\\ml2\\lib\\site-packages (from PyQt5) (12.13.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement PySide2 (from versions: none)\n",
      "ERROR: No matching distribution found for PySide2\n"
     ]
    }
   ],
   "source": [
    "!pip install tk\n",
    "!pip install PyQt5\n",
    "!pip install PySide2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.patches as patches\n",
    "import scipy\n",
    "\n",
    "# for visualization windows to pop out in jupyter (kernel may require restart after using GUIs)\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Tasks\n",
    "---\n",
    "* In all tasks, you should document your process and results in a report file (which will be saved as `.pdf`). \n",
    "* You can reference your code in the report file, but no need for actual code in this file, the code is submitted in a seprate folder as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/external-itim2101-lineal-color-itim2101/64/000000/external-robot-engineering-itim2101-lineal-color-itim2101.png\" style=\"height:50px;display:inline\"> Introduction \n",
    "---\n",
    "One of the major areas of computer vision is 3D reconstruction. Given several 2D images of an environment, can we recover the 3D structure of the environment, as well as the position of the camera/robot? This has many uses in robotics and autonomous systems, as understanding the 3D structure of the environment is crucial to navigation. You don't want your robot constantly bumping into walls, or running over human beings!\n",
    "\n",
    "<center> <img src=\"https://research.qut.edu.au/qcr/wp-content/uploads/sites/305/2021/02/Challenge_summary_pic-768x513.jpg\" style=\"height:300px\">\n",
    "<center> Image Source - <a href=\"https://research.qut.edu.au/qcr/2021/02/17/2nd-robotic-vision-scene-understanding-challenge-launched-cvpr2021-embodied-ai-workshop/\"> CVPR 21 Embodied AI Workshop </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1, you will be writing a set of functions to generate a sparse point cloud for some test images we have provided to you. The test images are 2 renderings of a temple from two different angles. We have also provided you with a `npz` file containing good point correspondences between the two images. You will first write a function that computes the fundamental matrix between the two images. Then write a function that uses the epipolar constraint to find more point matches between the two images. Finally, you will write a function that will triangulate the 3D points for each pair of 2D point correspondences.\n",
    "\n",
    "In Part 2, you will be writing a set of functions to calibrate a camera and project a 3D CAD model to a 2D image after estimating the camera pose. We have provided you with a `npz` file containing corresponding 2D-3D pairs. You will first write a function that estimates a camera matrix given 2D-3D calibration points. Then write a function to decompose the estimated camera matrix to intrinsic/extrinsic parameters. Finally, you will write a script to project the provided 3D CAD model and compare it to a given 2D image of an airplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/pastel-glyph/64/000000/pain-points.png\" style=\"height:50px;display:inline\"> Part 1 - Sparse Reconstruction \n",
    "---\n",
    "In this section, you will be writing a set of function to compute the sparse reconstruction from two sample images of a temple. You will first estimate the Fundamental matrix, compute point correspondences, then plot the results in 3D.\n",
    "It may be helpful to read through Section 1.5 right now. In Section 1.5 we ask you to write a testing script that will run your whole pipeline. It will be easier to start that now and add to it as you complete each of the questions one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Eight Point Algorithm\n",
    "---\n",
    "In this question, you're going to use the eight point algorithm which is covered in class to estimate the fundamental matrix. Please use the point correspondences provided in `data/some_corresp.npz`; you can load and view the contents of a `.npz` file as follows:\n",
    "> ``data = np.load(\"./Users/yiftachedelstain/ee046746-computer-vision-private/HW/hw4_structure_from_motion/data/some_corresp.npz\")``\n",
    "<br> `` print(data.files)`` </br>\n",
    "\n",
    "* Write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eight_point(pts1, pts2, pmax):\n",
    "    \"\"\"\n",
    "    Eight Point Algorithm\n",
    "    [I] pts1, points in image 1 (Nx2 matrix)\n",
    "        pts2, points in image 2 (Nx2 matrix)\n",
    "        pmax, scalar value computed as max(H1,W1)\n",
    "    [O] F, the fundamental matrix (3x3 matrix)\n",
    "    \"\"\"\n",
    "    # replace pass by your implementation\n",
    "\n",
    "    # Define the normalization matrix\n",
    "    T = np.array([[1/pmax, 0, 0],\n",
    "                  [0, 1/pmax, 0],\n",
    "                  [0, 0, 1]])\n",
    "\n",
    "    # Construct the A matrix\n",
    "    normalized_pts1 = np.array([(T @ np.append(pt, 1))[:2] for pt in pts1])\n",
    "    normalized_pts2 = np.array([(T @ np.append(pt, 1))[:2] for pt in pts2])\n",
    "    A = np.zeros((pts1.shape[0], 9))\n",
    "    for i in range(pts1.shape[0]):\n",
    "        x1, y1 = normalized_pts1[i]\n",
    "        x2, y2 = normalized_pts2[i]\n",
    "        A[i] = [x2*x1, x1*y2, x1, y1*x2, y1*y2, y1, x2, y2, 1]\n",
    "\n",
    "    # Solve for the Fundamental matrix\n",
    "    _, _, V = la.svd(A)\n",
    "    F = V[-1].reshape(3,3)\n",
    "\n",
    "    # Enforce rank 2 constraint\n",
    "    F = refineF(F, normalized_pts1, normalized_pts2)\n",
    "    return T.T @ F @ T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `pts1` and `pts2` are $N \\times 2$ matrices corresponding to the $(x,y)$ coordinates of the $N$ points in the first and second image respectively, and `pmax` is a scale parameter. Implementation tips:\n",
    "* Normalize points and un-normalize $F$: You should scale the data by dividing each coordinate by $p_{\\text{max}}$ (the maximum of the image's width and height) using a transformation matrix $T$. After computing $F$, you will have to \"unscale\" the fundamental matrix. If $p_{\\text{norm}} = Tp$, then $F_{\\text{unnorm}} = T^T F T$. Note that this scaling is slightly simpler than \"centering\" that you did in the lecture, but for the purpose of this assingment it should suffice.\n",
    "\n",
    "* You must enforce the rank 2 constraint on $F$ before unscaling. Recall that a valid fundamental matrix $F$ will have all epipolar lines intersect at a certain point, meaning that there exists a non-trivial null space for $F$. In general, with real points, the eight-point solution for $F$ will not come with this condition. To enforce the rank 2 constraint, decompose $F$ with SVD to get the three matrices $U,\\Sigma,V$ such that $F = U\\Sigma V^T$. Then force the matrix to be rank 2 by setting the smallest singular value in $\\Sigma$ to zero, giving you a new $\\Sigma'$. Now compute the proper fundamental matrix with $F' = U\\Sigma' V^T$.\n",
    "\n",
    "* You may find it helpful to refine the solution by using local minimization. This probably won't fix a completely broken solution, but may make a good solution better by locally minimizing a geometric cost function. For this we have provided a helper function `refineF` taking in $F$ and the two sets of points, which you can call from `eight_point` before unscaling $F$.\n",
    "\n",
    "* Remember that the x-coordinate of a point in the image is its column entry and y-coordinate is the row entry. Also note that eight-point is just a \"figurative\" name, it just means that you need at least 8 points; your algorithm should use an over-determined system ($N > 8$ points).\n",
    "\n",
    "* To visualize the correctness of your estimated $F$, use the provided function `displayEpipolarF`, which takes in $F$, and the two images. This GUI lets you select a point in one of the images and visualize the corresponding epipolar line in the other image (Figure 1).\n",
    "\n",
    "*  Please include in your report the recovered $F$ and the visualization of some epipolar lines (similar to Figure 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 1: singualrizes F using SVD\n",
    "def _singularize(F):\n",
    "    U, S, V = np.linalg.svd(F)\n",
    "    S[-1] = 0\n",
    "    F = U.dot(np.diag(S).dot(V))\n",
    "\n",
    "    return F\n",
    "\n",
    "# helper function 2.1: defines an objective function using F and the epipolar constraint\n",
    "def _objective_F(f, pts1, pts2):\n",
    "    F = _singularize(f.reshape([3, 3]))\n",
    "    num_points = pts1.shape[0]\n",
    "    hpts1 = np.concatenate([pts1, np.ones([num_points, 1])], axis=1)\n",
    "    hpts2 = np.concatenate([pts2, np.ones([num_points, 1])], axis=1)\n",
    "    Fp1 = F.dot(hpts1.T)\n",
    "    FTp2 = F.T.dot(hpts2.T)\n",
    "\n",
    "    r = 0\n",
    "    for fp1, fp2, hp2 in zip(Fp1.T, FTp2.T, hpts2):\n",
    "        r += (hp2.dot(fp1))**2 * (1/(fp1[0]**2 + fp1[1]**2) + 1/(fp2[0]**2 + fp2[1]**2))\n",
    "\n",
    "    return r\n",
    "\n",
    "# helper function 2.2: refines F using the objective from above and local optimization\n",
    "def refineF(F, pts1, pts2):\n",
    "    f = scipy.optimize.fmin_powell(\n",
    "        lambda x: _objective_F(x, pts1, pts2), F.reshape([-1]),\n",
    "        maxiter=100000,\n",
    "        maxfun=10000\n",
    "    )\n",
    "\n",
    "    return _singularize(f.reshape([3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualization functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 3.1: derives the epipoles using the essential matrix\n",
    "def _epipoles(E):\n",
    "    U, S, V = np.linalg.svd(E)\n",
    "    e1 = V[-1, :]\n",
    "    U, S, V = np.linalg.svd(E.T)\n",
    "    e2 = V[-1, :]\n",
    "\n",
    "    return e1, e2\n",
    "\n",
    "# helper function 3.2: GUI that uses F to draw the epipolar lines in I2 correponding to chosen pts in I1\n",
    "def displayEpipolarF(I1, I2, F):\n",
    "    e1, e2 = _epipoles(F)\n",
    "\n",
    "    sy, sx, _ = I2.shape\n",
    "\n",
    "    f, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 9))\n",
    "    ax1.imshow(I1)\n",
    "    ax1.set_title('Select a point in this image')\n",
    "    ax1.set_axis_off()\n",
    "    ax2.imshow(I2)\n",
    "    ax2.set_title('Verify that the corresponding point \\n is on the epipolar line in this image')\n",
    "    ax2.set_axis_off()\n",
    "\n",
    "    while True:\n",
    "        plt.sca(ax1)\n",
    "        x, y = plt.ginput(1, mouse_stop=2)[0]\n",
    "\n",
    "        xc, yc = int(x), int(y)\n",
    "        v = np.array([[xc], [yc], [1]])\n",
    "\n",
    "        l = F @ v\n",
    "        s = np.sqrt(l[0]**2+l[1]**2)\n",
    "\n",
    "        if s==0:\n",
    "            error('Zero line vector in displayEpipolar')\n",
    "\n",
    "        l = l / s\n",
    "        if l[1] != 0:\n",
    "            xs = 0\n",
    "            xe = sx - 1\n",
    "            ys = -(l[0] * xs + l[2]) / l[1]\n",
    "            ye = -(l[0] * xe + l[2]) / l[1]\n",
    "        else:\n",
    "            ys = 0\n",
    "            ye = sy - 1\n",
    "            xs = -(l[1] * ys + l[2]) / l[0]\n",
    "            xe = -(l[1] * ye + l[2]) / l[0]\n",
    "\n",
    "        ax1.plot(x, y, '*', linewidth=2) # changed from MarkerSize to markersize\n",
    "        ax2.plot([xs, xe], [ys, ye], linewidth=2)\n",
    "        plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example result:\n",
    "\n",
    "<center> <img src = \"../assets/displayEpipolarF_example.jpg\" style=\"width:75%\">\n",
    "<center> Figure 1 - Epipolar lines visualization from \"displayEpipolarF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pts1', 'pts2']\n"
     ]
    }
   ],
   "source": [
    "data = np.load('../data/some_corresp.npz')\n",
    "print(data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000106\n",
      "         Iterations: 55\n",
      "         Function evaluations: 5545\n",
      "Fundamental Matrix: \n",
      " [[-1.59040537e-09  1.20994575e-07 -2.86914854e-06]\n",
      " [ 6.79860135e-08  4.68494930e-11 -1.13237811e-03]\n",
      " [-1.61329373e-05  1.08910349e-03  4.44256004e-03]]\n"
     ]
    }
   ],
   "source": [
    "pts1, pts2 = data['pts1'], data['pts2']\n",
    "im1 = cv.cvtColor(cv.imread('../data/im1.png'), cv.COLOR_BGR2RGB)\n",
    "im2 = cv.cvtColor(cv.imread('../data/im2.png'), cv.COLOR_BGR2RGB)\n",
    "p_max = max(im1.shape[0], im1.shape[1])\n",
    "F = eight_point(pts1, pts2, p_max)\n",
    "print(\"Fundamental Matrix: \\n\", F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEpipolarF(im1, im2, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Epipolar Correpondences\n",
    "---\n",
    "To reconstruct a 3D scene with a pair of two images, we need to find many point pairs. A point pair is two points (one in each image) that correspond to the same 3D scene point. With enough of these pairs, when we plot the resulting 3D points, we will have a rough outline of the 3D object. You found point pairs in HW1 using feature detectors and feature descriptors, and testing a point in one image with every single point in the other image. But here we can use the fundamental matrix to greatly simplify this search.\n",
    "\n",
    "<center> <img src = \"../assets/epipolar_theory.jpg\" style=\"width:50%\">\n",
    "<center> Figure 2 - Epipolar Geometry. Potential matches for $p$ lie on the epipolar line $l'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from class that given a point $p$ in one image (the left view in Figure 2). Its corresponding 3D scene point $P$ could lie anywhere along the line from the camera center $C$ to the point $p$. This line, along with a second image's camera center $C'$ (the right view in Figure 2) forms a plane. This plane intersects with the image plane of the second camera, resulting in a line $l'$ in the second image which describes all the possible locations that $p$ may be found in the second image. Line $l'$ is the epipolar line, and we only need to search along this line to find a match for point $p$ found in the first image.\n",
    "\n",
    "* Write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epipolar_correspondences(I1, I2, F, pts1):\n",
    "    \"\"\"\n",
    "    Epipolar Correspondences\n",
    "    [I] I1, image 1 (H1xW1 matrix)\n",
    "        I2, image 2 (H2xW2 matrix)\n",
    "        F, fundamental matrix from image 1 to image 2 (3x3 matrix)\n",
    "        pts1, points in image 1 (Nx2 matrix)\n",
    "    [O] pts2, points in image 2 (Nx2 matrix)\n",
    "    \"\"\"\n",
    "    window_radius = 10  # Half of the window size\n",
    "    pts2 = []\n",
    "\n",
    "    # Iterate over each point in the first image\n",
    "    for pt in pts1:\n",
    "        x1, y1 = pt\n",
    "        # Define a search window around the point in the first image\n",
    "        patch1 = I1[max(0, y1 - window_radius):min(I1.shape[0], y1 + window_radius + 1),\n",
    "                    max(0, x1 - window_radius):min(I1.shape[1], x1 + window_radius + 1)]\n",
    "\n",
    "        # Compute the corresponding epipolar line in the second image\n",
    "        line = F @ np.array([x1, y1, 1])\n",
    "        a, b, c = line\n",
    "\n",
    "        best_match = None\n",
    "        lowest_distance = float('inf')\n",
    "\n",
    "        # Search along the epipolar line in the second image\n",
    "        for x2 in range(I2.shape[1]):\n",
    "            # Calculate the y-coordinate on the epipolar line \n",
    "            # ax + by + c = 0 => y = (-ax - c) / b\n",
    "            if b != 0:\n",
    "                y2 = int(-(a * x2 + c) / b)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Ensure y2 is within image bounds\n",
    "            if 0 <= y2 < I2.shape[0]:\n",
    "                # Extract a patch from the second image\n",
    "                patch2 = I2[max(0, y2 - window_radius):min(I2.shape[0], y2 + window_radius + 1),\n",
    "                            max(0, x2 - window_radius):min(I2.shape[1], x2 + window_radius + 1)]\n",
    "\n",
    "                # Compare patch sizes before calculating distance\n",
    "                if patch1.shape == patch2.shape:\n",
    "                    # Compute the Euclidean distance between patches\n",
    "                    distance = np.linalg.norm(patch1.astype(np.int32) - patch2.astype(np.int32))\n",
    "                    # Update the best match if a smaller distance is found\n",
    "                    if distance < lowest_distance:\n",
    "                        lowest_distance = distance\n",
    "                        best_match = (x2, y2)\n",
    "\n",
    "        pts2.append(best_match)\n",
    "    return np.array(pts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts1 shape: (110, 2), pts2 shape: (110, 2)\n"
     ]
    }
   ],
   "source": [
    "pts2 = epipolar_correspondences(im1, im2, F, pts1)\n",
    "print(f\"pts1 shape: {pts1.shape}, pts2 shape: {pts2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `I1` and `I2` are two-view images, `F` is the fundamental matrix computed for the two images using your `eight_point` function, `pts1` is a $N \\times 2$ matrix containing the $(x,y)$ points in the first image, and the function should return `pts2`, a $N \\times 2$ matrix, which contains the corresponding points in the second image. Implementation tips:\n",
    "\n",
    "* To match one point $p$ in image 1, use the fundamental matrix to estimate the corresponding epipolar line $l'$ and generate a set of candidate points in the second image.\n",
    "\n",
    "* For each candidate points $p'$, a similarity score between $p$ and $p'$ is computed. The point among candidates with highest score is treated as epipolar correspondence.\n",
    "\n",
    "* There are many ways to define the similarity between two points. Feel free to use whatever you want and **describe it in your write-up**. One possible solution is to select a small window of size $w$ around the point $p$. Then compare this target window to the window of the candidate point in the second image. For the images we gave you, simple Euclidean distance or Manhattan distance should suffice.\n",
    "\n",
    "* Remember to take care of data type and index range. You can use the provided function `epipolarMatchGUI` to visually test your function. Your function does not need to be perfect, but it should get most easy points correct, like corners, dots etc.\n",
    "\n",
    "* Please include a screenshot of `epipolarMatchGUI` running with your implementation of `epipolar_correspondences` (similar to Figure 3). Mention the similarity metric you decided to use. Also comment on any cases where your matching algorithm consistently fails, and why you might think this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 4: GUI that uses F, and the matching function to draw the epipolar correpondences on the epipolar lines\n",
    "def epipolarMatchGUI(I1, I2, F):\n",
    "    e1, e2 = _epipoles(F)\n",
    "\n",
    "    sy, sx, sd = I2.shape\n",
    "\n",
    "    f, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 9))\n",
    "    ax1.imshow(I1)\n",
    "    ax1.set_title('Select a point in this image')\n",
    "    ax1.set_axis_off()\n",
    "    ax2.imshow(I2)\n",
    "    ax2.set_title('Verify that the corresponding point \\n is on the epipolar line in this image')\n",
    "    ax2.set_axis_off()\n",
    "\n",
    "    while True:\n",
    "        plt.sca(ax1)\n",
    "        x, y = plt.ginput(1, mouse_stop=2)[0]\n",
    "\n",
    "        xc, yc = int(x), int(y)\n",
    "        v = np.array([[xc], [yc], [1]])\n",
    "\n",
    "        l = F @ v\n",
    "        s = np.sqrt(l[0]**2+l[1]**2)\n",
    "\n",
    "        if s==0:\n",
    "            error('Zero line vector in displayEpipolar')\n",
    "\n",
    "        l = l / s\n",
    "        if l[0] != 0:\n",
    "            xs = 0\n",
    "            xe = sx - 1\n",
    "            ys = -(l[0] * xs + l[2]) / l[1]\n",
    "            ye = -(l[0] * xe + l[2]) / l[1]\n",
    "        else:\n",
    "            ys = 0\n",
    "            ye = sy - 1\n",
    "            xs = -(l[1] * ys + l[2]) / l[0]\n",
    "            xe = -(l[1] * ye + l[2]) / l[0]\n",
    "\n",
    "        ax1.plot(x, y, '*', linewidth=2)\n",
    "        ax2.plot([xs, xe], [ys, ye], linewidth=2)\n",
    "\n",
    "        # draw points\n",
    "        pc = np.array([[xc, yc]])\n",
    "        p2 = epipolar_correspondences(I1, I2, F, pc)\n",
    "        ax2.plot(p2[0,0], p2[0,1], 'ro', linewidth=2)\n",
    "        plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example result:\n",
    "\n",
    "<center> <img src = \"../assets/epipolarMatchGUI_example.jpg\" style=\"width:75%\">\n",
    "<center> Figure 3 - Epipolar Match visualization. A few errors are alright, but it should get most easy points correct (corners, dots, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epipolarMatchGUI(im1, im2, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Essential Matrix\n",
    "---\n",
    "In order to get the full camera projection matrices we need to compute the Essential matrix. So far, we have only been using the Fundamental matrix.\n",
    "\n",
    "* Write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essential_matrix(F, K1, K2):\n",
    "    \"\"\"\n",
    "    Essential Matrix\n",
    "    [I] F, the fundamental matrix (3x3 matrix)\n",
    "        K1, camera matrix 1 (3x3 matrix)\n",
    "        K2, camera matrix 2 (3x3 matrix)\n",
    "    [O] E, the essential matrix (3x3 matrix)\n",
    "    \"\"\"\n",
    "    E = K2.T @ F @ K1\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `F` is the Fundamental matrix computed between two images, `K1` and `K2` are the intrinsic camera matrices for the first and second image respectively (contained in `data/intrinsics.npz`), and `E` is the computed essential matrix. The intrinsic camera parameters are typically acquired through camera calibration. Refer to the class slides for the\n",
    "relationship between the Fundamental matrix and the Essential matrix.\n",
    "\n",
    "* Please include your estimated $E$ matrix for the temple image pair in the PDF report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essential Matrix: \n",
      " [[-3.67640676e-03  2.80704796e-01  4.03209637e-02]\n",
      " [ 1.57726080e-01  1.09082992e-04 -1.69651547e+00]\n",
      " [ 2.58404922e-04  1.71769667e+00  1.97646817e-03]]\n"
     ]
    }
   ],
   "source": [
    "intrinsics_data = np.load(\"../data/intrinsics.npz\")\n",
    "K1, K2 = intrinsics_data['K1'], intrinsics_data['K2']\n",
    "E = essential_matrix(F, K1, K2)\n",
    "print(\"Essential Matrix: \\n\", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 - Triangulation\n",
    "---\n",
    "Write a function to triangulate pairs of 2D points in the images to a set of 3D points.\n",
    "\n",
    "* Write the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate(M1, pts1, M2, pts2):\n",
    "    \"\"\"\n",
    "    Triangulation\n",
    "    [I] M1, camera projection matrix 1 (3x4 matrix)\n",
    "        pts1, points in image 1 (Nx2 matrix)\n",
    "        M2, camera projection matrix 2 (3x4 matrix)\n",
    "        pts2, points in image 2 (Nx2 matrix)\n",
    "    [O] pts3d, 3D points in space (Nx3 matrix)\n",
    "    \"\"\"\n",
    "    pts3d = []\n",
    "    for point1, point2 in zip(pts1, pts2):\n",
    "        x1, y1 = point1\n",
    "        x2, y2 = point2\n",
    "        A = np.array([\n",
    "            y1 * M1[2] - M1[1],\n",
    "            M1[0] - x1 * M1[2],\n",
    "            y2 * M2[2] - M2[1],\n",
    "            M2[0] - x2 * M2[2]\n",
    "        ])\n",
    "        _, _, V = np.linalg.svd(A)\n",
    "        P_homogenous = V[-1]\n",
    "        P = P_homogenous[:3] / P_homogenous[3]\n",
    "        pts3d.append(P)\n",
    "    return np.array(pts3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `pts1` and `pts2` are the $N\\times 2$ matrices with the 2D image coordinates, `M1` and `M2` are the $3\\times4$ camera projection matrices and `pts3d` is an $N\\times 3$ matrix with the corresponding 3D points (in all cases, one point per row). Remember that you will need to multiply the given intrinsic matrices with your solution for the extrinsic camera matrices to obtain the final camera projection matrices. \n",
    "\n",
    "* For `M1` you can assume no rotation or translation, so the extrinsic matrix is just $\\left[I \\lvert 0\\right]$. \n",
    "\n",
    "* For `M2`, pass the essential matrix to the provided function `camera2` to get four possible **extrinsic** matrices. You will need to determine which of these is the correct one to use. Refer to the class slides to remind yourself of the 4 possible camera layouts. The correct configuration is the one for which most of the 3D points are in front of both cameras (i.e. have a positive depth), and make up a reasonable recovered shape. To make sure you choose the right one, write a helper function that apply the extrinsics of camera 2 to the recovered 3D points and check that they are still in front of camera 1 (positive $Z$).\n",
    "\n",
    "* Keep in mind to multiply the extrinsics matrices by the corresponding intrinsics before inputting them to `triangulate`.\n",
    "* Once implemented, check the performance by looking at the re-projection error. To compute the re-projection error, project the estimated 3D points back to the image 1 and compute the mean Euclidean error between projected 2D points and the given `pts1`.\n",
    "\n",
    "* **In your write-up**: Describe how you determined which extrinsic matrix is correct. Note that simply rewording the hint is not enough. Report your re-projection error using the given `pts1` and `pts2` in `data/some_corresp.npz`. If implemented correctly, the re-projection error should be less than 2 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 5: returns the 4 options for (EXTRINSIC) camera matrix M2 given the essential matrix\n",
    "def camera2(E):\n",
    "    U,S,V = np.linalg.svd(E)\n",
    "    m = S[:2].mean()\n",
    "    E = U.dot(np.array([[m,0,0], [0,m,0], [0,0,0]])).dot(V)\n",
    "    U,S,V = np.linalg.svd(E)\n",
    "    W = np.array([[0,-1,0], [1,0,0], [0,0,1]])\n",
    "\n",
    "    if np.linalg.det(U.dot(W).dot(V))<0:\n",
    "        W = -W\n",
    "\n",
    "    M2s = np.zeros([3,4,4])\n",
    "    M2s[:,:,0] = np.concatenate([U.dot(W).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
    "    M2s[:,:,1] = np.concatenate([U.dot(W).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
    "    M2s[:,:,2] = np.concatenate([U.dot(W.T).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
    "    M2s[:,:,3] = np.concatenate([U.dot(W.T).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
    "\n",
    "    return M2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_M2(M2s, K2, M1, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Choose optimal M2 extrinsic matrix from the 4 options.\n",
    "    We define the optimal M2 as the one that maximizes the number of visible points in both images, and minimizes the error.\n",
    "    We define the error as the sum of the Euclidean distance between the projected points and the actual points in both images.\n",
    "    [I] M2s, the 4 options for M2 (3x4x4 matrix)\n",
    "        K2, camera intrinsic matrix 2 (3x3 matrix)\n",
    "        M1, camera projection matrix 1 (3x4 matrix)\n",
    "        pts1, points in image 1 (Nx2 matrix)\n",
    "        pts2, points in image 2 (Nx2 matrix)\n",
    "    [O] optimal_M2_extrinsic, the optimal M2 extrinsic matrix (3x4 matrix)\n",
    "    \"\"\"\n",
    "    max_visible_points = -1\n",
    "    min_error = float('inf')\n",
    "    optimal_M2_extrinsic = None\n",
    "    for i in range(M2s.shape[2]):\n",
    "        M2_extrinsic = M2s[:, :, i]\n",
    "        M2_candidate = K2 @ M2_extrinsic\n",
    "        pts3d = triangulate(M1, pts1, M2_candidate, pts2)\n",
    "        pts3d_homogeneous = np.concatenate([pts3d, np.ones((pts3d.shape[0], 1))], axis=1)\n",
    "\n",
    "        pts1_projected = M1 @ pts3d_homogeneous.T\n",
    "        visible_pts1 = np.where(pts1_projected[2] > 0) # visible points in image 1\n",
    "        pts1_projected = pts1_projected[:2] / pts1_projected[2] # back to heterogeneous coordinates\n",
    "        pts2_projected = M2_candidate @ pts3d_homogeneous.T\n",
    "        visible_pts2 = np.where(pts2_projected[2] > 0) # visible points in image 2\n",
    "        pts2_projected = pts2_projected[:2] / pts2_projected[2] # back to heterogeneous coordinates\n",
    "        \n",
    "        visible_pts = np.intersect1d(visible_pts1, visible_pts2)\n",
    "        error = np.linalg.norm(pts1[visible_pts] - pts1_projected.T[visible_pts]) + np.linalg.norm(pts2[visible_pts] - pts2_projected.T[visible_pts])\n",
    "\n",
    "        if len(visible_pts) > max_visible_points: # we want as as many visible points in both images as possible\n",
    "            optimal_M2_extrinsic = M2_extrinsic\n",
    "            max_visible_points = len(visible_pts)\n",
    "            min_error = error\n",
    "        elif len(visible_pts) == max_visible_points and error < min_error: # if the number of visible points is the same, we want the smallest error\n",
    "            optimal_M2_extrinsic = M2_extrinsic\n",
    "            min_error = error\n",
    "\n",
    "    return optimal_M2_extrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M2 = \n",
      "[[ 1.39479171e+03 -3.54919763e+01  6.75507188e+02 -1.47099462e+03]\n",
      " [-2.68068672e+01  1.52580437e+03  2.46004114e+02  4.36824836e+00]\n",
      " [-2.51905111e-01  1.34124919e-03  9.67751009e-01  1.63420802e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the first camera's extrinsic matrix\n",
    "M1_extrinsic = np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "M1 = K1 @ M1_extrinsic\n",
    "\n",
    "E = essential_matrix(F, K1, K2)\n",
    "M2s = camera2(E)\n",
    "pts1, pts2 = data['pts1'], data['pts2']\n",
    "optimal_M2_extrinsic = choose_M2(M2s, K2, M1, pts1, pts2)\n",
    "\n",
    "# Compute the second camera's projection matrix\n",
    "M2 = K2 @ optimal_M2_extrinsic\n",
    "\n",
    "# Output the result\n",
    "print(\"M2 = \")\n",
    "print(M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 - Putting It All Together\n",
    "---\n",
    "You now have all the pieces you need to generate a full 3D reconstruction. Write a test script `test_temple_coords` that does the following:\n",
    "\n",
    "* Load the two images and the point correspondences from `data/some_corresp.npz`.\n",
    "* Run eight point to compute the fundamental matrix `F`.\n",
    "* Load the points in image 1 contained in `data/temple_coords.npz` and run your `epipolar_correspondences` on them to get the corresponding points in image 2.\n",
    "* Load `data/intrinsics.npz` and compute the essential matrix `E`.\n",
    "* Compute the first camera projection matrix `M1` and use `camera2` to compute the four candidates for `M2`.\n",
    "* Run your `triangulate` function using the four sets of camera matrix candidates (after scaling with the intrinsics), the points from `data/temple_coords.npz`, and their computed correspondences.\n",
    "* Figure out the correct `M2` and the corresponding 3D points.\n",
    "* Use matplotlib's scatter function to plot these point correspondences on screen.\n",
    "* Report your computed extrinsic parameters (`R1`,`R2`,`t1`,`t2`) in your PDF.\n",
    "* We will use your test script to run your code, so be sure it runs smoothly. In particular, use relative paths to load files, not absolute paths.\n",
    "* **In your write-up**: Include 3 images of your final reconstruction of the points given in the file `data/temple_coords.npz`, from different angles as shown in Figure 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example result:\n",
    "\n",
    "<center> <img src = \"../assets/pointcloud_example.jpg\" style=\"width:75%\">\n",
    "<center> Figure 4 - Sample Reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000106\n",
      "         Iterations: 55\n",
      "         Function evaluations: 5545\n"
     ]
    }
   ],
   "source": [
    "# 1: Load correspondence data and images\n",
    "correspondence_data = np.load(\"../data/some_corresp.npz\")\n",
    "pts1, pts2 = correspondence_data['pts1'], correspondence_data['pts2']\n",
    "im1 = cv.cvtColor(cv.imread(\"../data/im1.png\"), cv.COLOR_BGR2RGB)\n",
    "im2 = cv.cvtColor(cv.imread(\"../data/im2.png\"), cv.COLOR_BGR2RGB)\n",
    "max_dimension = np.max(im1.shape)\n",
    "\n",
    "# 2: Compute Fundamental matrix\n",
    "F = eight_point(pts1, pts2, max_dimension)\n",
    "\n",
    "# 3: Load pts1 from temple_coords.npz and find epipolar correspondences to im2\n",
    "temple_data = np.load(\"../data/temple_coords.npz\")\n",
    "temple_pts1 = temple_data['pts1']\n",
    "temple_pts2 = epipolar_correspondences(im1, im2, F, temple_pts1)\n",
    "\n",
    "# 4: Load intrinsics and compute Essential matrix\n",
    "camera_intrinsics = np.load(\"../data/intrinsics.npz\")\n",
    "K1, K2 = camera_intrinsics['K1'], camera_intrinsics['K2']\n",
    "E = essential_matrix(F, K1, K2)\n",
    "\n",
    "# 5: Compute M1 and find possible M2 extrinsic candidates\n",
    "M1_extrinsic = np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "M1 = K1 @ M1_extrinsic\n",
    "M2_extrinsic_candidates = camera2(E)\n",
    "\n",
    "# 6-7: Run triangulate on all possible M2 candidates and choose the best one, then find the 3D correspondences\n",
    "M2_extrinsic = choose_M2(M2_extrinsic_candidates, K2, M1, temple_pts1, temple_pts2)\n",
    "M2 = K2 @ M2_extrinsic\n",
    "temple_pts3d = triangulate(M1, temple_pts1, M2, temple_pts2)\n",
    "\n",
    "# 8: plot the 3D points using matplotlib scatter\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(temple_pts3d[:, 0], temple_pts3d[:, 1], temple_pts3d[:, 2], c='r')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('3D points of the temple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera 1 extrinsic parameters:\n",
      "R1 = \n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "t1 = [0. 0. 0.]\n",
      "Camera 2 extrinsic parameters:\n",
      "R2 = \n",
      "[[ 0.96747412 -0.02361054  0.25186576]\n",
      " [ 0.02318694  0.99972033  0.00464999]\n",
      " [-0.25190511  0.00134125  0.96775101]]\n",
      "t2 = [-1.         -0.02357654  0.1634208 ]\n"
     ]
    }
   ],
   "source": [
    "# 9: Report extrinsic parameters of both cameras\n",
    "print(\"Camera 1 extrinsic parameters:\")\n",
    "print(f\"R1 = \\n{M1_extrinsic[:, :3]}\")\n",
    "print(f\"t1 = {M1_extrinsic[:, 3]}\")\n",
    "\n",
    "print(\"Camera 2 extrinsic parameters:\")\n",
    "print(f\"R2 = \\n{M2_extrinsic[:, :3]}\")\n",
    "print(f\"t2 = {M2_extrinsic[:, 3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10: Save 3 images of the temple reconstruction from different angles\n",
    "save_path = \"../output/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "angles = [(30, 90), (90, 0), (45, 45)]\n",
    "for i, angle in enumerate(angles):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(temple_pts3d[:, 0], temple_pts3d[:, 1], temple_pts3d[:, 2], c='r')\n",
    "    ax.view_init(*angle)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.set_title(f'Temple 3D reconstruction at elevation {angle[0]} and azimuth {angle[1]}')\n",
    "    plt.savefig(f\"{save_path}temple_reconstruction_{i}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/external-ddara-lineal-ddara/64/000000/external-yoga-pose-yoga-poses-ddara-lineal-ddara-36.png\" style=\"height:50px;display:inline\"> Part 2 - Pose Estimation (BONUS)\n",
    "---\n",
    "In this section, you will implement what you have learned in class to estimate the intrinsic and extrinsic parameters of camera given 2D points $p$ on image and their corresponding 3D points $P$. In other words, given a set of matched points $\\left\\{P_i ,p_i\\right\\}$ and camera model (in homogeneous coordinates):\n",
    "\n",
    "$$ p = M P$$\n",
    "\n",
    "we want to find the estimate of the camera matrix $M\\in \\mathbb{R}^{3\\times4}$, as well as intrinsic parameter\n",
    "matrix $K\\in \\mathbb{R}^{3\\times3}$, camera rotation $R\\in \\mathbb{R}^{3\\times3}$ and camera translation $t\\in \\mathbb{R}^{3\\times1}$, such that:\n",
    "$$ M = K \\left[R \\lvert t\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Estimating $M$ (BONUS)\n",
    "---\n",
    "* Write a function that estimates the camera matrix $M$ given 2D and 3D points $p,P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find $M$ such that $p = M P$. In our possession we have $N$ points, so we can write this as a linear system of equations:\n",
    "$$ \\forall i \\in \\left\\{1,2,...,N\\right\\} \\quad p_i = M P_i $$\n",
    "As we saw in tutorial 8, this can be written as the following linear system:\n",
    "$$ \\begin{bmatrix} P_1^T & 0^T & -x_1 P_1^T \\\\\n",
    "0^T & P_1^T & -y_1 P_1^T \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    " P_N^T & 0^T & -x_N P_N^T \\\\\n",
    "0^T & P_N^T & -y_N P_N^T \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} m_{11} \\\\ m_{12} \\\\ \\vdots \\\\ m_{34} \\end{bmatrix} = 0 \n",
    "$$\n",
    "Where $m_{ij}$ are the elements of $M$. This can be written as $A\\mathbf{m} = 0$ where $A$ is the matrix on the left and $\\mathbf{m}$ is the vector of $m_{ij}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pose(p, P):\n",
    "    \"\"\"\n",
    "    Camera Matrix Estimation\n",
    "    [I] p, 2D points (Nx2 matrix)\n",
    "        P, 3D points (Nx3 matrix)\n",
    "    [O] M, camera matrix (3x4 matrix)\n",
    "    \"\"\"\n",
    "    N = p.shape[0]\n",
    "    A = np.zeros((2 * N, 12))\n",
    "    for i in range(N):\n",
    "        X, Y, Z = P[i]\n",
    "        x, y = p[i]\n",
    "        A[2 * i] = [X, Y, Z, 1, 0, 0, 0, 0, -x * X, -x * Y, -x * Z, -x]\n",
    "        A[2 * i + 1] = [0, 0, 0, 0, X, Y, Z, 1, -y * X, -y * Y, -y * Z, -y]\n",
    "    \n",
    "    _, _, V = np.linalg.svd(A)\n",
    "    M = V[-1].reshape(3, 4)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `p` is $2\\times N$ matrix denoting the $(x, y)$ coordinates of the $N$ points on the image plane and `P` is $3\\times N$ matrix denoting the $(x,y,z)$ coordinates of the corresponding points in the 3D world. Recall that this camera matrix can be computed using the same strategy as homography estimation by Direct Linear Transform (DLT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once you finish this function, you can run the following script to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojection Error with clean 2D points: 9.93494604501067e-11\n",
      "Pose Error with clean 2D points: 1.629023195469143e-12\n",
      "Reprojection Error with noisy 2D points: 4.136386447757975\n",
      "Pose Error with noisy 2D points: 1.0535173493310883\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate random camera matrix\n",
    "K = np.array([[1,0,100], [0,1,100], [0,0,1]])\n",
    "R, _,_ = la.svd(np.random.randn(3,3))\n",
    "if la.det(R) < 0: R = -R\n",
    "t = np.vstack((np.random.randn(2,1), 1))\n",
    "M = K @ np.hstack((R, t))\n",
    "\n",
    "# 2. Generate random 2D and 3D points\n",
    "N = 100\n",
    "P = np.random.randn(N,3)\n",
    "p = M @ np.hstack((P, np.ones((N,1)))).T\n",
    "p = p[:2,:].T / np.vstack((p[2,:], p[2,:])).T\n",
    "\n",
    "# 3. Test pose estimation with clean points\n",
    "Mc = estimate_pose(p, P)\n",
    "pp = Mc @ np.hstack((P, np.ones((N,1)))).T\n",
    "pp = pp[:2,:].T / np.vstack((pp[2,:], pp[2,:])).T\n",
    "\n",
    "print('Reprojection Error with clean 2D points:', la.norm(p-pp))\n",
    "print('Pose Error with clean 2D points:', la.norm((Mc/Mc[-1,-1])-(M/M[-1,-1])))\n",
    "\n",
    "# 4. Test pose estimation with noisy points\n",
    "p = p + np.random.rand(p.shape[0], p.shape[1])\n",
    "Mn = estimate_pose(p, P)\n",
    "pp = Mn @ np.hstack((P, np.ones((N,1)))).T\n",
    "pp = pp[:2,:].T / np.vstack((pp[2,:], pp[2,:])).T\n",
    "\n",
    "print('Reprojection Error with noisy 2D points:', la.norm(p-pp))\n",
    "print('Pose Error with noisy 2D points:', la.norm((Mn/Mn[-1,-1])-(M/M[-1,-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **In your write-up**: Please include the output of this script in your PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Intrinsic/Extrinsic Parameters (BONUS)\n",
    "---\n",
    "* Write a function that estimates both intrinsic and extrinsic parameters from a camera matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in tutorial 8: \n",
    "$$ M = [N | -Nc] $$\n",
    "Where $N = KR$, $K$ is an upper triangular matrix and $R$ is orthogonal. We can decompose $N$ using QR decomposition to get $K$ and $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_params(M):\n",
    "    \"\"\"\n",
    "    Camera Parameter Estimation\n",
    "    [I] M, camera matrix (3x4 matrix)\n",
    "    [O] K, camera intrinsics (3x3 matrix)\n",
    "        R, camera extrinsics rotation (3x3 matrix)\n",
    "        t, camera extrinsics translation (3x1 matrix)\n",
    "    \"\"\"\n",
    "    # Find camera center c\n",
    "    _, _, V = la.svd(M)\n",
    "    c = V[-1]\n",
    "    c = c / c[-1]\n",
    "\n",
    "    # As we saw in tutorial 8: M = [N|-Nc] where N = KR\n",
    "    N = M[:, :3]\n",
    "    # in RQ decomposition, R is upper triangular and Q is orthogonal (in our caseK is upper triangular, R is orthogonal)\n",
    "    K, R = scipy.linalg.rq(N) \n",
    "    t = -R @ c[:3]\n",
    "    return K, R, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we learned on class, the `estimate_params` function should consecutively run the following steps:\n",
    "* Compute the camera center $c$ by using SVD. Hint: $c$ is the eigenvector corresponding to the smallest eigenvalue.\n",
    "* Compute the intrinsic $K$ and rotation $R$ by using QR decomposition. $K$ is a right upper triangle matrix while $R$ is an orthonormal matrix.\n",
    "* Compute the translation by $t = -Rc$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once you finish this function, you can run the following script to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrinsic Error with clean 2D points: 3.748479535111032e-12\n",
      "Rotation Error with clean 2D points: 1.7207644396375664e-13\n",
      "Translation Error with clean 2D points: 2.9785389273603022\n",
      "Intrinsic Error with noisy 2D points: 0.6319586469132118\n",
      "Rotation Error with noisy 2D points: 0.020119290825631034\n",
      "Translation Error with noisy 2D points: 2.977361445730394\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate random camera matrix\n",
    "K = np.array([[1,0,100], [0,1,100], [0,0,1]])\n",
    "R, _,_ = la.svd(np.random.randn(3,3))\n",
    "if la.det(R) < 0: R = -R\n",
    "t = np.vstack((np.random.randn(2,1), 1))\n",
    "M = K @ np.hstack((R, t))\n",
    "\n",
    "# 2. Generate random 2D and 3D points\n",
    "N = 100\n",
    "P = np.random.randn(N,3)\n",
    "p = M @ np.hstack((P, np.ones((N,1)))).T\n",
    "p = p[:2,:].T / np.vstack((p[2,:], p[2,:])).T\n",
    "\n",
    "# 3. Test parameter estimation with clean points\n",
    "Mc = estimate_pose(p, P)\n",
    "Kc, Rc, tc = estimate_params(Mc)\n",
    "\n",
    "print('Intrinsic Error with clean 2D points:', la.norm((Kc/Kc[-1,-1])-(K/K[-1,-1])))\n",
    "print('Rotation Error with clean 2D points:', la.norm(R-Rc))\n",
    "print('Translation Error with clean 2D points:', la.norm(t-tc))\n",
    "\n",
    "# 4. Test parameter estimation with noisy points\n",
    "p = p + np.random.rand(p.shape[0], p.shape[1])\n",
    "Mn = estimate_pose(p, P)\n",
    "Kn, Rn, tn = estimate_params(Mn)\n",
    "\n",
    "print('Intrinsic Error with noisy 2D points:', la.norm((Kn/Kn[-1,-1])-(K/K[-1,-1])))\n",
    "print('Rotation Error with noisy 2D points:', la.norm(R-Rn))\n",
    "print('Translation Error with noisy 2D points:', la.norm(t-tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **In your write-up**: Please include the output of this script in your PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Projecting CAD Models to 2D (BONUS)\n",
    "---\n",
    "Now you will utilize what you have implemented to estimate the camera matrix from a real image, shown in Figure 5 (left), and project the 3D object (CAD model), shown in Figure 5 (right), back on to the image plane. \n",
    "\n",
    "<center> <img src = \"../assets/airplane_image_CAD.jpg\" style=\"width:75%\">\n",
    "<center> Figure 5 - The provided image and 3D CAD model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a script that does the following:\n",
    "* Load an image `image`, a CAD model `cad`, 2D points `x` and 3D points `X` from the data file `data/pnp.npz`.\n",
    "* Run `estimate_pose` and `estimate_params` to estimate camera matrix $M$, intrinsic matrix $K$, rotation matrix $R$, and translation $t$.\n",
    "* Use your estimated camera matrix $M$ to project the given 3D points $P$ onto the image.\n",
    "* Plot the given 2D points $p$ and the projected 3D points on screen. An example is shown in Figure 6 (left).\n",
    "* Draw the CAD model rotated by your estimated rotation $R$ on screen. An example is shown in Figure 6 (middle).\n",
    "* Project the CAD's all vertices onto the image and draw the projected CAD model overlapping with the 2D image. An example is shown in Figure 6 (right).\n",
    "* **In your write-up**: Please include the three images similar to Figure 6 in your PDF. You must use different colors from Figure 6. For example, green circle for given 2D points, black points for projected 3D points, blue CAD model, and red projected CAD model overlapping on the image. You will get **NO credit** if you use the same color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Load the data from the file\n",
    "data_content = np.load(\"../data/pnp.npz\", allow_pickle=True)\n",
    "image, cad, x, X = data_content['image'], data_content['cad'], data_content['x'], data_content['X']\n",
    "\n",
    "# 2: Estimate the M and then K, R, t\n",
    "M = estimate_pose(x, X)\n",
    "K, R, t = estimate_params(M)\n",
    "\n",
    "# 3: Project the 3D points onto the 2D image plane using M\n",
    "X_homogenous = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "projected_X = M @ X_homogenous.T\n",
    "projected_X /= projected_X[2]\n",
    "projected_X_2d = projected_X[:2].T\n",
    "\n",
    "# 4: Plot the projected 3D points onto the 2D image plane overlaid with the 2D image\n",
    "plt.imshow(image)\n",
    "\n",
    "# Plot the projected 3D points in black\n",
    "plt.scatter(projected_X_2d[:, 0], projected_X_2d[:, 1], c='black', marker='o', label='Projected 3D Points', s=10)\n",
    "\n",
    "# Draw green circles around the given 2D points\n",
    "for coord in x:\n",
    "    circle_patch = patches.Circle((coord[0], coord[1]), radius=10, edgecolor='g', facecolor='none', label='Given 2D Points')\n",
    "    plt.gca().add_patch(circle_patch)\n",
    "plt.legend(['Projected 3D Points', 'Given 2D Points'])\n",
    "plt.title('Projected 3D Points onto the 2D Image Plane')\n",
    "plt.savefig(f\"{save_path}projected_3d_points.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Plot the CAD model rotated by the estimated R\n",
    "cad_vertices = cad[0][0][0]\n",
    "cad_faces = cad[0][0][1]\n",
    "\n",
    "# Rotate the CAD model vertices by the estimated R\n",
    "rotated_cad_vertices = (R @ cad_vertices.T).T\n",
    "\n",
    "# Plot the rotated CAD model\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot_trisurf(rotated_cad_vertices[:, 0], rotated_cad_vertices[:, 1], rotated_cad_vertices[:, 2], triangles=cad_faces-1, edgecolor='blue')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('Rotated CAD Model')\n",
    "plt.savefig(f\"{save_path}rotated_cad_model.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Project CAD model onto the 2D image plane using M and plot the projected CAD model overlaid with the 2D image\n",
    "# Project the CAD model onto the 2D image plane using M\n",
    "cad_homogenous = np.hstack((cad_vertices, np.ones((cad_vertices.shape[0], 1))))\n",
    "projected_cad = M @ cad_homogenous.T\n",
    "projected_cad /= projected_cad[2, :]\n",
    "projected_cad_2d = projected_cad[:2, :].T\n",
    "\n",
    "# Plot the projected CAD model onto the 2D image plane overlaid with the 2D image\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.imshow(image)\n",
    "ax.triplot(projected_cad_2d[:, 0], projected_cad_2d[:, 1], triangles=cad_faces-1, c='red', label='Projected CAD Model', alpha=0.5)\n",
    "plt.legend(['Projected CAD Model'])\n",
    "plt.title('Projected CAD Model onto 2D Image Plane')\n",
    "plt.savefig(f\"{save_path}projected_cad_model.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example result:\n",
    "\n",
    "<center> <img src = \"../assets/airplane_projection.jpg\" style=\"width:100%\">\n",
    "<center> Figure 6 - Project a CAD model back onto the image. Left: the image annotated with given 2D points (blue circle) and projected 3D points (red points); Middle: the CAD model rotated by estimated R; Right: the image overlapping with\n",
    "projected CAD model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> References & Credits\n",
    "* Carnegie Mellon University - CMU\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
